# radiology-clip-mini

Minimal CLIP-style baseline on **Open-I (IU X-Ray)** (~50 paired cases) for image‚Üîreport retrieval with a single activation heatmap (‚ÄúGrad-CAM-style‚Äù) example.

## TL;DR

* **Image encoder:** ResNet-18 (ImageNet) ‚Üí MLP head ‚Üí **256-D**
* **Text encoder:** DistilBERT ‚Üí MLP head ‚Üí **256-D**
* **Loss:** symmetric InfoNCE with learnable temperature œÑ
* **Eval:** Recall@1/5/10, Median Rank, nDCG@10
* **Hardware:** GTX-1650 or Kaggle T4; RAM ‚â• 8 GB

> **Why this repo (NKI fit):** compact, reproducible **vision‚Äìlanguage retrieval** baseline with patient-wise splits and honest metrics‚Äîexactly the kind of multimodal building block to scale to oncology CT/MRI (Project 2), and a stepping stone toward 3D encoders (Project 1).

## Repo layout

```
radiology-clip-mini/
‚îú‚îÄ configs/      # YAML configs (explicit hyper-params)
‚îú‚îÄ notebooks/    # thin wrappers calling library code
‚îú‚îÄ results/      # run outputs (metrics, ckpts, figs) ‚Äî git-ignored
‚îú‚îÄ src/rclip/    # library code (data, models, train, eval, viz)
‚îî‚îÄ scripts/      # small utilities
```

## Quickstart (local, Bash)

```bash
git clone https://github.com/MahshadSa/radiology-clip-mini.git
cd radiology-clip-mini
python -m venv .venv && source .venv/bin/activate   # Windows below
pip install -r requirements.txt

# Train (writes results/<YYYYMMDD-HHMMSS>/ and updates results/latest.txt)
python -m rclip.train --config configs/tiny.yaml

# Evaluate + visualise using the pointer above
python -m rclip.eval --ckpt "$(cat results/latest.txt)"
python -m rclip.viz  --ckpt "$(cat results/latest.txt)"
```

**Windows (PowerShell)**

```powershell
python -m venv .venv; .\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
python -m rclip.train --config configs/tiny.yaml
$ckpt = Get-Content results/latest.txt
python -m rclip.eval --ckpt "$ckpt"
python -m rclip.viz  --ckpt "$ckpt"
```

**Kaggle Notebook (Bash cells)**

```bash
git clone https://github.com/MahshadSa/radiology-clip-mini.git
cd radiology-clip-mini
pip install -r requirements.txt
python -m rclip.train --config configs/tiny.yaml
CKPT=$(cat results/latest.txt)
python -m rclip.eval --ckpt "$CKPT"
python -m rclip.viz  --ckpt "$CKPT"
```

## Data

* **Source:** Open-I (IU X-Ray) via ü§ó Datasets: `"ykumards/open-i"`.
* **Text priority:** findings ‚Üí impression ‚Üí report ‚Üí text (case-insensitive, cleaned).
* **Split policy:** **patient-level** (prevents leakage).
* **Subset:** ~50 patients for speed (configurable in `configs/tiny.yaml`).

## Configuration (example)

```yaml
seed: 42
data:
  dataset: openi
  max_patients: 50
  val_frac: 0.1
  test_frac: 0.1
  cache_dir: data/hf
train:
  epochs: 8
  batch_size: 16
  lr: 1.0e-3
  weight_decay: 1.0e-4
  grad_clip: 1.0
  amp: true
model:
  embed_dim: 256
  text_encoder: distilbert-base-uncased
  temperature_init: 0.07
paths:
  results_dir: results
```

## Results (thumbnail)

* *Image‚ÜíText:* R@1 ***x.xx***, R@5 ***x.xx***, R@10 ***x.xx***, MedR ***x***, nDCG@10 ***x.xx***
* *Text‚ÜíImage:* R@1 ***x.xx***, R@5 ***x.xx***, R@10 ***x.xx***, MedR ***x***, nDCG@10 ***x.xx***

![retrieval grid](viz/retrieval_grid.png)

> Notes: short generic phrases (‚Äúno acute abnormality‚Äù) over-match; activation maps are qualitative, not localisation.

## Repro notes

* Deterministic seeds; `results/<run>/run.json` stores config + git hash.
* Metrics in `results/<run>/metrics.json`.
* Figures in `viz/` (e.g., `retrieval_grid.png`, `gradcam_example.png`).

## Lessons learned

* Reports are messy. normalising headers/case helped zero-shot retrieval.
* Patient-wise splits matter. file-wise inflated R@k.
* Tokeniser/versions can mismatch.  lock DistilBERT and assert lengths.

## Cite

* Radford et al., *Learning Transferable Visual Models From Natural Language Supervision (CLIP)*, 2021.
* Demner-Fushman et al., *Preparing a Collection of Radiology Examinations for Distribution and Retrieval*, 2016 (IU X-Ray).

## License & ethics

MIT. Research/education only; not for clinical use.
