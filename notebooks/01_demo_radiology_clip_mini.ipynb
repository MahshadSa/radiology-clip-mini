{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92e94362",
   "metadata": {},
   "source": [
    "# Radiology CLIP Mini – Quick Demo\n",
    "\n",
    "This notebook shows a small end-to-end demo for **radiology-clip-mini**:\n",
    "\n",
    "1. Load config and a trained checkpoint.\n",
    "2. Build dataloaders and the CLIPMini model.\n",
    "3. Compute retrieval metrics (image → text, text → image).\n",
    "4. Display a retrieval grid for one query.\n",
    "5. Display a GradCAM-style overlay for one validation image.\n",
    "\n",
    "> This notebook assumes:\n",
    "> - You are running it from the `notebooks/` folder in the repo.\n",
    "> - You have already trained a small model via:\n",
    ">   `python -m rclip.train --config configs/tiny.yaml`\n",
    "> - There is a valid checkpoint path in `results/latest.txt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1127846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "# Ensure we can import the package from src/\n",
    "ROOT = pathlib.Path.cwd().parent  # repo root (notebooks/ -> root)\n",
    "SRC = ROOT / \"src\"\n",
    "sys.path.insert(0, str(SRC))\n",
    "\n",
    "from rclip.data import build_dataloaders\n",
    "from rclip.models import CLIPMini\n",
    "from rclip.eval import recall_at_k, ndcg_at_k  # from your eval module\n",
    "from rclip.viz import retrieval_grid, gradcam_last_block, overlay_heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d31a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = ROOT / \"configs\" / \"tiny.yaml\"\n",
    "with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "latest_path = (ROOT / \"results\" / \"latest.txt\")\n",
    "if not latest_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"results/latest.txt not found. \"\n",
    "        \"Run `python -m rclip.train --config configs/tiny.yaml` first.\"\n",
    "    )\n",
    "\n",
    "ckpt_path = pathlib.Path(latest_path.read_text().strip())\n",
    "ckpt_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39c7fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "dls = build_dataloaders(cfg)\n",
    "\n",
    "model = CLIPMini(\n",
    "    embed_dim=cfg[\"model\"][\"embed_dim\"],\n",
    "    text_model=cfg[\"model\"][\"text_encoder\"],\n",
    "    tau_init=cfg[\"model\"][\"temperature_init\"],\n",
    ").to(device)\n",
    "\n",
    "state = torch.load(ckpt_path, map_location=device)\n",
    "model.load_state_dict(state[\"model\"])\n",
    "model.eval();\n",
    "\n",
    "sum(p.numel() for p in model.parameters()) / 1e6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7889796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def embed_split(model, loader, device):\n",
    "    zs_img, zs_txt = [], []\n",
    "    for b in loader:\n",
    "        imgs = b[\"images\"].to(device, non_blocking=True)\n",
    "        toks = model.text_enc.tokenize(b[\"texts\"]).to(device)\n",
    "        _, zi, zt = model(imgs, toks)\n",
    "        zs_img.append(zi.cpu())\n",
    "        zs_txt.append(zt.cpu())\n",
    "    if not zs_img:\n",
    "        return None, None\n",
    "    return torch.cat(zs_img, dim=0), torch.cat(zs_txt, dim=0)\n",
    "\n",
    "\n",
    "zi_val, zt_val = embed_split(model, dls[\"val\"], device)\n",
    "if zi_val is None:\n",
    "    raise RuntimeError(\"Validation loader produced no embeddings\")\n",
    "\n",
    "sim = zi_val @ zt_val.t()\n",
    "\n",
    "it = recall_at_k(sim)\n",
    "it.update(ndcg_at_k(sim))\n",
    "\n",
    "ti = recall_at_k(sim.t())\n",
    "ti.update(ndcg_at_k(sim.t()))\n",
    "\n",
    "print(\"Image → Text:\", it)\n",
    "print(\"Text → Image:\", ti)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e2103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a query index in the validation set\n",
    "q = 0\n",
    "\n",
    "# similarity row for query q\n",
    "row = sim[q]\n",
    "topk = row.argsort(descending=True)[:5]  # top 5 texts for this image\n",
    "\n",
    "# we need the raw validation batch texts; easiest is to rerun over loader once\n",
    "texts_val = []\n",
    "for b in dls[\"val\"]:\n",
    "    texts_val.extend(b[\"texts\"])\n",
    "texts_val = texts_val[: sim.size(0)]\n",
    "\n",
    "print(\"Query image index:\", q)\n",
    "print(\"\\nTop 5 retrieved reports:\\n\")\n",
    "for rank, idx in enumerate(topk.tolist(), start=1):\n",
    "    print(f\"Rank {rank}:\")\n",
    "    print(texts_val[idx])\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05800e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_dir = ckpt_path.parent / \"viz\"\n",
    "viz_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "grid_path = viz_dir / \"retrieval_grid_notebook.png\"\n",
    "retrieval_grid(model, dls[\"val\"], device, grid_path, k=5)\n",
    "\n",
    "display(Image.open(grid_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e840c0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first image from first validation batch\n",
    "batch = next(iter(dls[\"val\"]))\n",
    "img = batch[\"images\"][0]\n",
    "\n",
    "heat = gradcam_last_block(model, img, device)\n",
    "overlay = overlay_heatmap(img, heat)\n",
    "\n",
    "overlay_path = viz_dir / \"gradcam_example_notebook.png\"\n",
    "overlay.save(overlay_path)\n",
    "\n",
    "display(overlay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72d28b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first image from first validation batch\n",
    "batch = next(iter(dls[\"val\"]))\n",
    "img = batch[\"images\"][0]\n",
    "\n",
    "heat = gradcam_last_block(model, img, device)\n",
    "overlay = overlay_heatmap(img, heat)\n",
    "\n",
    "overlay_path = viz_dir / \"gradcam_example_notebook.png\"\n",
    "overlay.save(overlay_path)\n",
    "\n",
    "display(overlay)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
